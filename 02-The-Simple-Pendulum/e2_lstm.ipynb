{"cells":[{"cellId":"57dbd1a7972249309261f421fbf86c47","cell_type":"markdown","metadata":{"id":"TKvYiJgnYExi","cell_id":"57dbd1a7972249309261f421fbf86c47","deepnote_cell_type":"markdown"},"source":"This notebook provides examples to go along with the [textbook](https://underactuated.csail.mit.edu/pend.html).  I recommend having both windows open, side-by-side!\n","block_group":"35ae656b7c10443c9d968c673cbdc450"},{"cellId":"a8507d59ce734bbda4bce1e939b0cf17","cell_type":"code","metadata":{"id":"A4QOaw_zYLfI","cell_id":"a8507d59ce734bbda4bce1e939b0cf17","deepnote_cell_type":"code"},"source":"import numpy as np\nfrom pydrake.all import StartMeshcat\n\nfrom underactuated.meshcat_utils import _interact","block_group":"d587a89bf8dc4b53bb311c9745852703","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"2cf5f27b93cf45f48067d9cff36c8767","cell_type":"code","metadata":{"cell_id":"2cf5f27b93cf45f48067d9cff36c8767","deepnote_cell_type":"code"},"source":"# Start the visualizer (run this cell only once, each instance consumes a port)\nmeshcat = StartMeshcat()","block_group":"a9df656627b7491ca365812e95b586ea","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"8714b65177e14ebebf1a05ba28aacfee","cell_type":"markdown","metadata":{"id":"CvoRbIK1KUSq","cell_id":"8714b65177e14ebebf1a05ba28aacfee","deepnote_cell_type":"markdown"},"source":"# Recurrent neural network units: LSTM and JANET\n\nJANET (Just Another NETwork) is a simplied version of the famous Long Short-Term Memory (LSTM) model as described in https://arxiv.org/abs/1804.04849 .  Relative to the autapse model, it adds a multiplicative \"forget gate\":\n$$\\dot{x} + x = f (1-\\alpha) x + (1-f)\\tanh(wx + u),\\\\ f = \\sigma(w_f x + u_f).$$\nI've written it here in continuous time and also added a small \"leak term\", $\\alpha>0$.  $\\sigma()$ is the sigmoid function, with range (0,1).  When $f=0$ we have the autapse model, where \"memory\" is possible.  When $f=1$ we have $\\dot{x} = -\\alpha x$, which \"forgets\" by implementing a dynamics with pushing the activations back towards zero.  ","block_group":"fc58aae21d3947548d2b2095ceb786cf"},{"cellId":"92c43b01ad8f45f98155f826d039dc55","cell_type":"code","metadata":{"cell_id":"92c43b01ad8f45f98155f826d039dc55","deepnote_cell_type":"code"},"source":"meshcat.DeleteAddedControls()\n\n\ndef sigma(x):\n    return 1.0 / (1 + np.exp(-x))\n\n\ndef janet(x, w, u, wf, uf, alpha):\n    f = sigma(wf * x + uf)\n    return -x + f * (1 - alpha) * x + (1 - f) * np.tanh(w * x + u)\n\n\nJanet = np.vectorize(janet)\nxmax = 2.0\nymax = 1.0\nx = np.arange(-xmax, xmax, 0.01)\n\nmeshcat.Delete()\nmeshcat.Set2dRenderMode(xmax=xmax, xmin=-xmax, ymin=-ymax, ymax=ymax)\nmeshcat.SetProperty(\"/Grid\", \"visible\", True)\nmeshcat.SetProperty(\"/Axes\", \"visible\", True)\n\n\ndef update(w=1, u=0, wf=0, uf=0, alpha=0.1):\n    # TODO(russt): Visualize fixed points here, too.\n    meshcat.SetLine(\n        \"janet\",\n        np.vstack([x, 0 * x, Janet(x, w=w, u=u, wf=wf, uf=uf, alpha=alpha)]),\n        line_width=4.0,\n    )\n\n\n_interact(\n    meshcat,\n    update,\n    w=(0, 3, 0.1),\n    u=(-1.5, 1.5, 0.1),\n    wf=(-2, 2, 0.1),\n    uf=(-5, 5, 0.1),\n    alpha=(0, 1, 0.02),\n)","block_group":"fa72020eb18242d398b9ed4717c0b9af","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"9f76b29ce8824621b4da3c0fb9468c57","cell_type":"code","metadata":{"cell_id":"9f76b29ce8824621b4da3c0fb9468c57","deepnote_cell_type":"code"},"source":"","block_group":"338971491fd447aba98785fc7c28a9fb","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null}],
        "metadata": {"deepnote_notebook_id":"00016ae8eabb48629061bea15d0677f3"},
        "nbformat": 4,
        "nbformat_minor": 0,
        "version": 0
      }